# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139ji4AIvX6uyZkZmMhJ9DABJjf4gBvfq

Implementación de SVM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import KFold, train_test_split

df = pd.read_csv('/content/audio_features.csv')

positive_samples = df[df['covid'] == 1]
negative_samples = df[df['covid'] == 0]

positive_samples_duplicated = positive_samples.sample(n=1057, replace=True, random_state=42)

df_balanced = pd.concat([negative_samples, positive_samples, positive_samples_duplicated]).reset_index(drop=True)
df_balanced = shuffle(df_balanced, random_state=42)

X = pd.DataFrame(df_balanced.drop(columns=['covid']))
Y = pd.Series(df_balanced['covid'])

# Asegurar que los índices de X y Y están restablecidos correctamente
X = X.reset_index(drop=True)
Y = Y.reset_index(drop=True)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

mean = X_train.mean(axis=0)
std = X_train.std(axis=0)
X_train_scaled = (X_train - mean) / std
X_test_scaled = (X_test - mean) / std

def loss(y, x, w, bias, c):
    m = np.maximum(0, 1 - y * (x @ w + bias))
    return 0.5 * np.dot(w, w) + c * np.sum(m)

def grad(y, x, w, bias, c):
    margin = y * (np.dot(x, w) + bias)
    incorrect_classified = (margin < 1).astype(int)
    grad_w = w - c * np.dot(x.T, (incorrect_classified * y))
    grad_b = -c * np.sum(incorrect_classified * y)
    return grad_w, grad_b

def update(w, b, grad, alpha):
    w -= grad[0] * alpha
    b -= grad[1] * alpha
    return w, b

def train_ovr_kfold(X, y, num_epochs, k, c=1000, alpha=0.00001):
    n_features = X.shape[1]
    n_classes = len(np.unique(y))

    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    total_train_losses = []
    total_val_losses = []

    for train_index, val_index in kf.split(X):
        X_train, X_val = X.iloc[train_index].values, X.iloc[val_index].values
        y_train, y_val = y.iloc[train_index].values, y.iloc[val_index].values

        weights = np.zeros((n_classes, n_features))
        biases = np.zeros(n_classes)

        train_losses = []
        val_losses = []

        for current_class in range(n_classes):
            binary_y_train = np.where(y_train == current_class, 1, -1)
            binary_y_val = np.where(y_val == current_class, 1, -1)

            w = np.random.rand(n_features)
            b = np.random.random()

            for epoch in range(num_epochs):
                grad_values = grad(binary_y_train, X_train, w, b, c)
                w, b = update(w, b, grad_values, alpha)

                train_loss = loss(binary_y_train, X_train, w, b, c)
                val_loss = loss(binary_y_val, X_val, w, b, c)

                train_losses.append(train_loss)
                val_losses.append(val_loss)

            weights[current_class, :] = w
            biases[current_class] = b

        total_train_losses.append(train_losses)
        total_val_losses.append(val_losses)

    return weights, biases, total_train_losses, total_val_losses

def predict_ovr(x, weights, biases):
    scores = x @ weights.T + biases
    return np.argmax(scores, axis=1)

def plot_loss(train_losses, val_losses):
    epochs = range(1, len(train_losses) + 1)
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, train_losses, label='Entrenamiento', color='blue')
    plt.plot(epochs, val_losses, label='Validación', color='red')
    plt.title('Pérdida vs. Épocas')
    plt.xlabel('Épocas')
    plt.ylabel('Pérdida')
    plt.legend()
    plt.grid(True)
    plt.savefig('SVM.png')

num_epochs = 2000
alpha = 0.1
C = 0.06
k = 5

weights, biases, train_losses, val_losses = train_ovr_kfold(X_train_scaled, y_train, num_epochs, k=k, c=C, alpha=alpha)

y_pred = predict_ovr(X_test_scaled, weights, biases)

correct = np.sum(y_pred == y_test)
incorrect = len(y_test) - correct
effectiveness = round(100 * correct / len(y_test), 2)

print("Clasificados correctamente:", correct)
print("Clasificados incorrectamente:", incorrect)
print("% de efectividad:", effectiveness)

conf_matrix = confusion_matrix(y_test, y_pred)
print("Matriz de confusión:")
print(conf_matrix)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matriz de Confusión SVM OvR con Balanceo de Clases')
plt.xlabel('Predicción')
plt.ylabel('Clase Real')
plt.show()

print(classification_report(y_test, y_pred))